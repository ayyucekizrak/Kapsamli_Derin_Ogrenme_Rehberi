{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KapsamlÄ± Derin Ã–ÄŸrenme Rehberi",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "oWmGQ8lI6s4S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "ğŸŒŸ # Derin Ã–ÄŸrenme (Deep Learning) KavramlarÄ± Ã–zet ğŸŒŸ\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Kaynaklar ğŸ“š\n",
        "**1:** [Deep Learning Specialization](https://www.deeplearning.ai/) (5 Kursun Ã¶zeti)\n",
        "\n",
        "**2:** [Deep Learning Drizzle](https://github.com/kmario23/deep-learning-drizzle#balloon-reinforcement-learning-hotsprings-video_game)\n",
        "\n",
        "**3:** [ÅU KARA KUTUYU AÃ‡ALIM: Yapay Sinir AÄŸlarÄ±](https://medium.com/deep-learning-turkiye/%C5%9Fu-kara-kutuyu-a%C3%A7alim-yapay-sinir-a%C4%9Flar%C4%B1-7b65c6a5264a)\n",
        "\n",
        "**4:** [DERÄ°NE DAHA DERÄ°NE: EvriÅŸimli Sinir AÄŸlarÄ±](https://medium.com/deep-learning-turkiye/deri%CC%87ne-daha-deri%CC%87ne-evri%C5%9Fimli-sinir-a%C4%9Flar%C4%B1-2813a2c8b2a9)\n",
        "\n",
        "**5:** [Stanford University: Convolutional Neural Networks cheatsheet](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)\n",
        "\n",
        "**6:** [Derin Ã–ÄŸrenme Ä°Ã§in Aktivasyon FonksiyonlarÄ±nÄ±n KarÅŸÄ±laÅŸtÄ±rÄ±lmasÄ±](https://medium.com/deep-learning-turkiye/derin-%C3%B6%C4%9Frenme-i%C3%A7in-aktivasyon-fonksiyonlar%C4%B1n%C4%B1n-kar%C5%9F%C4%B1la%C5%9Ft%C4%B1r%C4%B1lmas%C4%B1-cee17fd1d9cd)\n",
        "\n",
        "**7:** [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "\n",
        "**8:** [CS 230 - Derin Ã–ÄŸrenme](https://stanford.edu/~shervine/teaching/cs-230//)\n",
        "\n",
        "**9:** [Deep Learning A-Zâ„¢| Python ile Derin Ã–ÄŸrenme](https://github.com/ayyucekizrak/Udemy_DerinOgrenmeyeGiris)\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "-Tt6DgOD66sI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Bu Ã§alÄ±ÅŸma araÅŸtÄ±rmalar yaparken benzerlerine rastlayÄ±p iyileÅŸtirerek derlemeye Ã§alÄ±ÅŸtÄ±ÄŸÄ±m ve derin Ã¶ÄŸrenme (deep learning) konusunda kÄ±sa bir Ã¶zet ve bolca kaynak yÃ¶nlendirmesi olan (hatta sonunda koca bir liste var) hÄ±zlÄ±ca konuya giriÅŸ yapÄ±labilinmesi iÃ§in gereklilikleri Ã¶zetlemektedir. LÃ¼tfen katkÄ± vermekten Ã§ekinmeyin ğŸ‘½"
      ]
    },
    {
      "metadata": {
        "id": "bQXUG3Qk7Mvi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Bu not defterine neden ihtiyacÄ±nÄ±z olabilir? ğŸ•µ\n",
        "\n",
        "\n",
        "Derin Ã–ÄŸrenmede birÃ§ok parametre, hiperparametre ve konsept vardÄ±r. \n",
        "Bu alanda yeni olanlar iÃ§in bazÄ± temel konular Ã¼zerinden hÄ±zlÄ±ca geÃ§mek iÃ§in bu not defteri oluÅŸturulmuÅŸtur. \n",
        "\n",
        "Ã–zellikle derin Ã¶ÄŸrenme terimleriyle karÅŸÄ±laÅŸtÄ±ÄŸÄ±nÄ±zda ne anlama geldiÄŸini bilmeniz cÃ¼mle iÃ§inde ne anlatÄ±ldÄ±ÄŸÄ±nÄ± doÄŸru ÅŸekilde anlamanÄ±z iÃ§in kritik Ã¶nem arz etmektedir. Bu yÃ¼zden bu not defteri bir sÃ¶zlÃ¼k gibi de kullanabileceÄŸiniz bir kaynak olma niteliÄŸinde. \n"
      ]
    },
    {
      "metadata": {
        "id": "tZ1AxLTg_4wC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TeÅŸekkÃ¼r ğŸ™\n",
        "Coursera [DeepLearning.ai](https://www.deeplearning.ai/) derslerinde notasyonu temel alarak hazÄ±rlanmÄ±ÅŸtÄ±r. Bu dersi yayÄ±nlayan ekibe Ã§okÃ§a teÅŸekkÃ¼r ederim. \n",
        "\n",
        "** ğŸ…Ancak bu derslere TÃ¼rkÃ§e alt yazÄ± ekleme ortak Ã§alÄ±ÅŸmasÄ± ile TÃ¼rkiye'de derslerin takip edilmesine ve alanda geliÅŸmeye katkÄ± saÄŸladÄ±ÄŸÄ± iÃ§in** \n",
        "\n",
        "**[Deep Learning TÃ¼rkiye](https://medium.com/deep-learning-turkiye/t%C3%BCrk%C3%A7e-altyaz%C4%B1l%C4%B1-yapay-zeka-ve-derin-%C3%B6%C4%9Frenme-kursu-deeplearning-ai-85d60f4f29d7) Ã¼yelerine de Ã§ok teÅŸekkÃ¼rler.**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Zuwm_Yjp7Ycv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Notasyon Ã–zeti ğŸ“‘\n",
        "TÃ¼m not defterini takip ederken bu notasyon tanÄ±mlarÄ±nÄ± dikkate almalÄ±sÄ±nÄ±z.\n",
        "\n",
        "notasyon | tanÄ±m\n",
        "--- | ---\n",
        "$m$ | eÄŸitim Ã¶rneklerinin sayÄ±sÄ±\n",
        "$n_x$ | her bir eÄŸitim Ã¶rneÄŸi iÃ§in Ã¶zrniteliklerin sayÄ±sÄ± \n",
        "$X$ | giriÅŸ matrisi: her sÃ¼tun bir eÄŸitim Ã¶rneÄŸi \n",
        "$Y$ | Ã§Ä±kÄ±ÅŸ matrisi: her bir sÃ¼tun $X$ eÄŸitim setinin etiketi olarak tanÄ±mlanÄ±r. Ã–rneÄŸin: $X[0]$ 1. eÄŸitim Ã¶rneÄŸi iÃ§in $Y[0]$ etikettir\n",
        "$\\hat{Y}$ | yeni test girdileri iÃ§in tahmin edilen etiket\n",
        "$Z$ |  $X$ 'in doÄŸrusal dÃ¶nÃ¼ÅŸÃ¼mÃ¼\n",
        "$A$\t| $Z$ 'nin doÄŸrusal olmayan dÃ¶nÃ¼ÅŸÃ¼mÃ¼, aktivasyon fonksiyonu Ã§Ä±kÄ±ÅŸÄ±\n",
        "$W$ |$X$ 'deki her Ã¶znitelik iÃ§in aÄŸÄ±rlÄ±k matrisi\n",
        "$x$\t| bir eÄŸitim Ã¶rneÄŸinin Ã¶znitelikleri\n",
        "$y$\t| bir eÄŸitim Ã¶rneÄŸinin Ã§Ä±kÄ±ÅŸ etiketi\n",
        "$\\hat{y}$\t| bir eÄŸitim Ã¶rneÄŸinin tahmini Ã§Ä±kÄ±ÅŸ etiketi\n",
        "$z$\t| $x$ 'in doÄŸrusal dÃ¶nÃ¼ÅŸÃ¼mÃ¼\n",
        "$a$\t| $z$ 'nin doÄŸrusal olmayan dÃ¶nÃ¼ÅŸÃ¼mÃ¼, aktivasyon fonksiyonu Ã§Ä±kÄ±ÅŸÄ±\n",
        "$w$\t| $x$ iÃ§in aÄŸÄ±rlÄ±k matrisi\n",
        "$b$\t| bias matrisi\n",
        "$\\sigma$ | sigmoid fonksiyonu, $\\sigma(z)= \\frac{1}{(1 + e^{-z} )}$  ve $z$ 'nin herhangi bir deÄŸeri iÃ§in Ã§Ä±kÄ±ÅŸ deÄŸeri (0,1) aralÄ±ÄŸÄ±ndadÄ±r\n",
        "$x_j^{(i)}$ | i. eÄŸitim Ã¶rneÄŸi iÃ§in j Ã¶znitelik deÄŸeri \n",
        "$w_j^{(i)[k]}$ | j. gizli birimin k. katmanÄ±nÄ±n i. eÄŸitim Ã¶rneÄŸi iÃ§in aÄŸÄ±rlÄ±k deÄŸeri \n",
        "$L$ | derin sinir aÄŸÄ±ndaki toplam katman sayÄ±sÄ± (giriÅŸ katmanÄ± hariÃ§)\n",
        "$n^{[l]}$ | $l$ gizli katmanÄ±ndaki nÃ¶ron sayÄ±sÄ± \n",
        "$X^{\\{t\\}}$, $Y^{\\{t\\}}$ | mini-kÃ¼me gradyan (bayÄ±r) iniÅŸinde t. mini-kÃ¼me iÃ§in $X$ ve $Y$deÄŸerleri\n",
        "$J$ | tÃ¼m eÄŸitim Ã¶rnekleri sonucunda model tarafÄ±ndan hesaplanan maliyet fonksiyonu\n",
        "$C$\t| Ã§ok-sÄ±nÄ±f sÄ±nÄ±flayÄ±cÄ± iÃ§in sÄ±nÄ±f sayÄ±sÄ±\n",
        "$X^{(i)<t>}$ | sÄ±ralÄ± (dizi) modellerde, i. eÄŸitim Ã¶rneÄŸinde t. elemanÄ±n gÃ¶sterimi\n",
        "$Y^{(i)<t>}$ | sÄ±ralÄ± (dizi) modellerde, i. eÄŸitim Ã¶rneÄŸinde t. elemanÄ±n Ã§Ä±kÄ±ÅŸ deÄŸerinin gÃ¶sterimi\n",
        "$T_X^{(i)}$ | sÄ±ralÄ± (dizi) modellerde, i. eÄŸitim Ã¶rneÄŸi iÃ§in giriÅŸ dizisinin uzunluÄŸunun gÃ¶sterimi \n",
        "$T_Y^{(i)}$ | sÄ±ralÄ± (dizi) modellerde, i. eÄŸitim Ã¶rneÄŸi iÃ§in Ã§Ä±kÄ±ÅŸ dizisinin uzunluÄŸunun gÃ¶sterimi\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "cyt39ZFv_u1-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lojistik BaÄŸlanÄ±m (Regresyon), Logistic Regression, Derin Sinir AÄŸlarÄ±nÄ±n YapÄ± TaÅŸÄ± ğŸ’\n",
        "\n",
        "SÄ±nÄ±flanÄ±rma iÃ§in kulanÄ±lan doÄŸrusal bir modeldir. Modelin amacÄ± verilen giriÅŸe gÃ¶re Ã§Ä±kÄ±ÅŸ olasÄ±lÄ±klarÄ±nÄ±n tahmin etmektir. \n",
        "\n",
        "$$z= w^T x+b$$\n",
        "$$a=\\sigma(z)$$\n",
        "$$\\hat{y} = a$$\n",
        "\n",
        "Ne kadar iyi tahmin yapÄ±ldÄ±ÄŸÄ±nÄ± her bir eÄŸitim Ã¶rneÄŸi iÃ§in Ã¶lÃ§mek gerekir ve bunun iÃ§in Ã§apraz entropi kaybÄ± hesaplanmalÄ±dÄ±r. \n",
        "\n",
        "$$L(\\hat{y},y)= -(y log(â¡\\hat{y}) + (1-y)log(â¡\\hat{y}))$$\n",
        "\n",
        "TÃ¼m Ã¶rnekler iÃ§in maliyet fonksiyonu,\n",
        "$$J= -\\frac{1}{m} \\sum_{i=1}^mL(\\hat{y}^{(i)} ,y^{(i)})$$\n",
        "Buarada $J$ optimizasyon algoritmasÄ± (Ã¶rneÄŸin gradyan (bayÄ±r) iniÅŸ) tarafÄ±ndan kullanÄ±lÄ±r.Optimiazsyonun amacÄ± optimum $w$ ve $b$ deÄŸerlerini bulmaktÄ±r."
      ]
    },
    {
      "metadata": {
        "id": "CApcuF65BuPL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## SÄ±ÄŸ Sinir AÄŸlarÄ± \n",
        "\n",
        "Lojistik regresyonda $z$ ve $a$, her eÄŸitim Ã¶rneÄŸi iÃ§in tahmin elde etmek iÃ§in hesaplanÄ±r. SÄ±ÄŸ bir sinir aÄŸÄ±nda, bu iÅŸlem Ã§Ä±kÄ±ÅŸ etiketini tahmin etmek iÃ§in iki kez tekrarlanÄ±r. \n",
        "\n",
        "*Lojistik regresyon:*\n",
        "\n",
        "\n",
        "![logistic_regression_network](https://drive.google.com/uc?export=view&id=1dm9gVeDOf6FOZdaBhIHp94fkRPCP7_nN)\n",
        "\n",
        "\n",
        " *SÄ±ÄŸ sinir aÄŸÄ±:*\n",
        "\n",
        "![shallow_neural_network](https://drive.google.com/uc?export=view&id=1dFX5kBsG45RLvnAyFNl80TNdIN7UGizH)\n",
        "\n",
        "  \n",
        "\n",
        "[1] ve [2] aÄŸÄ±n katmanlarÄ±dÄ±r. [1] KatmanÄ± ne bir giriÅŸ ne de Ã§Ä±kÄ±ÅŸ katmanÄ±dÄ±r, gizli katman olarak tanÄ±mlanmaktadÄ±r. [1] KatmanÄ± 3 gizli birime (nÃ¶ron) sahiptir ve katman [2] 1 tane nÃ¶rona sahiptir.\n",
        "\n",
        "SÄ±ÄŸ sinir aÄŸÄ±nda $x$ eÄŸitim Ã¶rneÄŸi (giriÅŸ) iÃ§in tahmin aÅŸaÄŸÄ±daki gibi hesaplanmaktadÄ±r,\n",
        "\n",
        "\n",
        "$$z^{[1]}= w^{[1]} x+b^{[1]}$$\n",
        "$$a^{[1]} = \\sigma(z^{[1]})$$\n",
        "$$z^{[2]} = w^{[2]}a^{[1]} +b^{[2]}$$\n",
        "$$\\hat{y}=a^{[2]} =\\sigma(z^{[2]})$$\n",
        "\n",
        "Bu iÅŸlem tÃ¼m eÄŸitim Ã¶rnekleri iÃ§in genelleÅŸtirilirse: $Z^{[1]}$, $Z^{[2]}$, $A^{[1]}$, $A^{[2]}$, $\\hat{Y}$.  \n",
        "EÄŸer iÅŸlem 2'den daha fazla gizli katmana sahip olursa artÄ±k buna **derin sinir aÄŸÄ±** (deep neural nets) diyoruz!!!\n"
      ]
    },
    {
      "metadata": {
        "id": "gRTDte3UW3Dd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Aktivasyon fonksiyonlarÄ±\n",
        "\n",
        "- $sigmoid$, $\\sigma(z)= \\frac{1}{1+e^{-z}}$\n",
        "  - Ïƒ(z), (0, 1) aralÄ±ÄŸÄ±nda deÄŸerler alÄ±r.\n",
        "  - genellikle son katmanda ikili sÄ±nÄ±flandÄ±rma problemleri iÃ§in kullanÄ±lÄ±r.\n",
        "- $tanh(z)= \\frac{e^z  - e^{-z}}{e^z  + e^{-z}}$\n",
        "  - $tanh(z)$ (-1, 1) aralÄ±ÄŸÄ±nda deÄŸerler alÄ±r.\n",
        "  -  Sigmoid'ten farklÄ± olarak grafik 0 merkezlidir.\n",
        "- $ReLU(z) = max (0,z)$\n",
        "  - $z$ Ã§ok kÃ¼Ã§Ã¼k ya da Ã§ok bÃ¼yÃ¼k deÄŸerlere sahip olduÄŸunda sigmoid ve tanh Ã¶ÄŸrenmesi yavaÅŸlar. \n",
        "  - bu yÃ¼zden sigmoid ve tanh ile karÅŸÄ±laÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda modelin Ã¶ÄŸrenme hÄ±zÄ± ReLU'da daha yÃ¼ksektir.\n",
        "  - genellikle gizli katmanlarda kullanÄ±lmaktadÄ±r.\n",
        "- $Leaky\\ ReLU(z) = maxâ¡(0.01z,z)$\n",
        "\n",
        "ğŸ” **[Aktivasyon fonksiyonlarÄ± hakkÄ±nda daha detaylÄ± bilgi ve uygulama Ã¼zerinde performanslarÄ±nÄ± deÄŸerlendirmek isterseniz bu baÄŸlantÄ±dan devam edebilirsiniz!](https://medium.com/deep-learning-turkiye/derin-%C3%B6%C4%9Frenme-i%C3%A7in-aktivasyon-fonksiyonlar%C4%B1n%C4%B1n-kar%C5%9F%C4%B1la%C5%9Ft%C4%B1r%C4%B1lmas%C4%B1-cee17fd1d9cd)**"
      ]
    },
    {
      "metadata": {
        "id": "CnlEfi4HYPzG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Derin Sinir AÄŸlarÄ± ğŸ”¥\n",
        "\n",
        "BasitÃ§e sÃ¶ylemek gerekirse, Ã§oklu gizli katmanlara sahip bir sinir aÄŸÄ±dÄ±r. Katman sayÄ±sÄ± $L$ ve her katmandaki nÃ¶ron sayÄ±sÄ±, eÄŸitimden Ã¶nce karar verilen hiperparametrelerdir.\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1_qm8c14Gws-k_aR1zeBzlN2O0wvP1kgt\" height=\"250px\" alt=\"deep neural nets\" />\n",
        "</center>\n",
        "<caption>\n",
        "  <center>\n",
        "    <strong>Åekil 1: </strong>\n",
        "    4 KatmanlÄ± bir tam baÄŸlantÄ±lÄ±ÄŸÄ± derin sinir aÄŸÄ±\n",
        "  </center>\n",
        "</caption>\n",
        "\n",
        "YukarÄ±daki aÄŸ $L = 4$, $n^{[1]} = 3$, $n^{[2]} = 4$, $n^{[3]} = 3$ ve $n^{[4]}=1$ katmanlarÄ±ndan oluÅŸmaktadÄ±r. TÃ¼m eÄŸitim Ã¶rneklerinin sonucu $\\hat{Y} = A^L$'dir.  \n",
        "$X = A^{[0]}$ ÅÃ¶yle hesaplanÄ±r:\n",
        " \n",
        "\n",
        "$$Z^{[1]} = W^{[1]}  A^{[0]} + b^{[1]}$$\n",
        "\n",
        "$$A^{[1]} = g^{[1]}( Z^{[1]})$$\n",
        "\n",
        "Benzer ÅŸekilde [2], [3] ve [4] katmanlarÄ± iÃ§in de aynÄ± iÅŸlemler gerÃ§ekleÅŸtirilir.\n",
        "$$\\hat{Y}= A^{[L=4]}= g^{[4]}( Z^{[4]})$$\n",
        "\n",
        "\n",
        "Burada $g^{[l]}$, $l$ katmanÄ±nda kullanÄ±lan aktivasyon fonksiyonudur. `numpy` vektÃ¶rleri ile uygulandÄ±ÄŸÄ±nda, tÃ¼m hesaplamalar eÄŸitim Ã¶rnekleri arasÄ±nda paralelleÅŸtirilir ve buna vektÃ¶rleÅŸtirme (vectorized) adÄ± verilmektedir. VektÃ¶rleÅŸtirme olmadan, sinir aÄŸÄ±, bir eÄŸitim epochu (dÃ¶nemi) tamamlamak iÃ§in eÄŸitim Ã¶rneklerini birer birer dÃ¶ngÃ¼lendirmek zorundadÄ±r. Bu Ã¶ÄŸrenmeyi yavaÅŸlatÄ±r. \n",
        "\n",
        "\n",
        "Her eÄŸitim Ã¶rneÄŸi $x^{(i)}$, $\\hat{y}^{(i)}$ tahminini son katmandan elde etmek iÃ§in aÄŸdan geÃ§irilir. Bu adÄ±m, tÃ¼m sÃ¼reÃ§ boyunca **ileri yayÄ±lÄ±m** olarak adlandÄ±rÄ±lÄ±r. $\\hat{y}^{(i)}$, hata tahmini elde etmek iÃ§in $J$ kullanarak $y^{(i)}$ ile karÅŸÄ±laÅŸtÄ±rÄ±lÄ±r. Bu hata $[L]$ 'den $[L-1]$' e$[L-2]$ 'ye geri gÃ¶nderilir ve bÃ¶ylece $[1]$ 'e, her katmandaki $W^{[l]}$, $b^{[l]}$ 'yi ayarlamak iÃ§in tekrar uygulanÄ±r, bÃ¶ylece bir sonraki tahmin hatasÄ± daha kÃ¼Ã§Ã¼k olur. Bu hatayÄ± geri alma aÅŸamasÄ±, tÃ¼m sÃ¼reÃ§ boyunca **geriye yayÄ±lÄ±m** olarak adlandÄ±rÄ±lÄ±r. Hata her geri yayÄ±lmasÄ±nda, sistemin $W^{[l]}$, $b^{[l]}$ parametrelerinde yaptÄ±ÄŸÄ± deÄŸiÅŸiklik miktarÄ± Ã¶ÄŸrenme hÄ±zÄ±, $\\alpha$ adlÄ± bir hiperparametre tarafÄ±ndan yÃ¶netilir."
      ]
    },
    {
      "metadata": {
        "id": "RTdp6tJvicA2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### BoyutlarÄ±n kontrol edilmesi\n",
        "AÅŸaÄŸÄ±daki formÃ¼ller size bir derin sinir aÄŸÄ± modeli uyarlamasÄ± yaparken [matris boyutlarÄ±](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ndarray.shape.html)nÄ±zÄ±n doÄŸruluÄŸunu kontrol etmenizde yardÄ±mcÄ± olur: \n",
        "\n",
        "- $w^{[l]}.shape = (n^{[l]}, n^{[l-1]})$\n",
        "- $b^{[l]}.shape = (n^{[l]}, 1)$\n",
        "- $A^{[l]}.shape = Z^{[l]}.shape = (n^{[l]}, m)$\n",
        "\n",
        "*(shape: ÅŸekil, biÃ§im, durum / numpy fonksiyonu)*"
      ]
    },
    {
      "metadata": {
        "id": "xTONqYzKjHfX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Hiperparametrelerin seÃ§ilmesi\n",
        "$W^{[l]}$, $b^{[l]}$sinir aÄŸÄ±nÄ±n eÄŸitim aÅŸamasÄ±nda Ã¶ÄŸrenmeye Ã§alÄ±ÅŸtÄ±ÄŸÄ± parametrelerdir. \n",
        "\n",
        "Hiperparametreler, eÄŸitim Ã¶ncesi geliÅŸtirici tarafÄ±ndan manuel olarak ayarlanÄ±r (belli bir rastgeleliÄŸe de sahiptir).\n",
        "\n",
        "-  Ã¶ÄŸrenme hÄ±zÄ± alpha,  $\\alpha$  - tahminleri gerÃ§ek deÄŸerlere yaklaÅŸtÄ±rmak iÃ§in parametrelerin gÃ¼ncellenme hÄ±zÄ±\n",
        "- epoch sayÄ±sÄ± - TÃ¼m eÄŸitim verisiyle bir kez eÄŸitim yaptÄ±ktan sonra bir epoch tamamlanÄ±r. Bu parametre, bunun kaÃ§ kez tekrarlanmasÄ± gerektiÄŸini bulmamÄ±za yarar.\n",
        "- gizli katmanlar, L - Derin Sinir AÄŸÄ±nda (DNN) kaÃ§ tane gizli katman var\n",
        "- her bir gizli katmandaki nÃ¶ronlar - $n^{[1]}$, $n^{[2]}$, $n^{[3]}$,â€¦, $n^{[L]}$ deÄŸerleri iÃ§in\n",
        "- aktivasyon fonksiyonlarÄ± - her katmanda kullanÄ±lacak  aktivasyon fonksiyonu, $g^{[1]}$, $g^{[2]}$, $g^{[3]}$,â€¦, $g^{[L]}$"
      ]
    },
    {
      "metadata": {
        "id": "XZ6cdSdH8LEP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Derin Sinir AÄŸlarÄ±nÄ±n Optimizasyonu ğŸ¹\n",
        "\n",
        "### Video AnlatÄ±m iÃ§in baÄŸlantÄ±larÄ± ziyaret ediniz! \n",
        "* [Yapay Ã–ÄŸrenme Modelleri GeliÅŸtirirken KarÅŸÄ±laÅŸÄ±lan Sorunlar & Ã‡Ã¶zÃ¼mleri (Part1)](https://www.youtube.com/watch?v=gbzwtZGrkrQ&t=67s)\n",
        "* [Yapay Ã–ÄŸrenme Modelleri GeliÅŸtirirken KarÅŸÄ±laÅŸÄ±lan Sorunlar & Ã‡Ã¶zÃ¼mleri (Part2)](https://www.youtube.com/watch?v=L3NJi7diDmg&t=141s)"
      ]
    },
    {
      "metadata": {
        "id": "gEzV5faK67hu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Veri bÃ¶lme/ayÄ±rma - AynÄ± daÄŸÄ±lÄ±mdaki tÃ¼m veriler âœ‚ï¸\n",
        "KullanÄ±labilir tÃ¼m etiketlenmiÅŸ veriler\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1XNwp6CzbTE0LM1UxpPPeXAzPLZmFqpIH\" alt=\"data splitting in same distribution\" />\n",
        "\n",
        "-\t**EÄŸitim verisi** (Train data) â€“ Verinin bÃ¼yÃ¼k bir kÄ±smÄ± eÄŸitim iÃ§in kullanÄ±lÄ±r\n",
        "-\t**DoÄŸrulama/GeÃ§erleme verisi** (Dev data) â€“ Modelin doÄŸruluÄŸunu kontrol etmek ve hiÄŸerparametrelerde ayarlama yapabilmek iÃ§in kullanÄ±lÄ±r. \n",
        "-\t**Test verisi** (Test data) â€“ Son olarak karar verilen modeli doÄŸrulamak/ test etmek iÃ§in kullanÄ±lÄ±r\n"
      ]
    },
    {
      "metadata": {
        "id": "5tgmguu78Gc_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Hata TÃ¼rleri âŒ\n",
        "\n",
        "Åekil 2'de gÃ¶sterildiÄŸi gibi, derin sinir aÄŸlarÄ± (Deep Neural Nets-DNN) test hatasÄ±nÄ±n yanÄ±nda eÄŸitim ve doÄŸrulama/geÃ§erleme hatasÄ±na da sahip.\n",
        "\n",
        "- Ã–nlenebilir bias â€“ insan hatasÄ± ile eÄŸtim hatasÄ± arasÄ±ndaki fark olarak tanÄ±mlanÄ±r. Bunu azaltmanÄ±n olasÄ± Ã§Ã¶zÃ¼mleri:\n",
        "    - Daha bÃ¼yÃ¼k bir aÄŸÄ± eÄŸitmek ($L$ ya da $n^{[l]}$artÄ±rmak)\n",
        "    - Epoch sayÄ±sÄ±nÄ± artÄ±rmak\n",
        "    - AÄŸ mimarisini deÄŸiÅŸtirmek\n",
        "- Varyans â€“ eÄŸitim hatasÄ± ve doÄŸrulama hatasÄ± arasÄ±ndaki fark olarak tanÄ±mlanÄ±r. difference between training error and dev error. Bu eÄŸitim verisinin aÅŸÄ±rÄ± uydurulmasÄ±nda (overfitting) meydana gelir. Bunu azaltmanÄ±n olasÄ± Ã§Ã¶zÃ¼mleri:\n",
        "    - Daha fazla veri ile eÄŸitmek\n",
        "    - Regularizasyon/ DÃ¼zenleme\n",
        "    - AÄŸ mimarisini deÄŸiÅŸtirmek\n",
        "    \n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1jLAPA4kUOxzd6KWqrnTPelJhCPsVqRy7\" alt=\"errors when all data is from same distribution\" />\n",
        "</center>\n",
        "<center>\n",
        "  <caption>\n",
        "    <strong>Åekil 2: </strong>\n",
        "    Her hatanÄ±n aralÄ±ÄŸÄ±\n",
        "  </caption>\n",
        "</center>"
      ]
    },
    {
      "metadata": {
        "id": "tKy-BC7u_k_V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Veri bÃ¶lme/ayÄ±rma â€“ FarklÄ± daÄŸÄ±lÄ±mlara sahip veriler âœ‚ï¸\n",
        "\n",
        "Ä°deal olarak, eÄŸitim, doÄŸrulama ve test kÃ¼meleri en iyi sonuÃ§ iÃ§in aynÄ± veri daÄŸÄ±lÄ±mÄ±ndan olmalÄ±dÄ±r. Ancak bazen derin Ã¶ÄŸrenme deneyi yapmak iÃ§in yeterince bÃ¼yÃ¼k veri bulunmayabilir. Ã–rneÄŸin, 2 kedinizin 100 resmini sÄ±nÄ±flandÄ±rmak iÃ§in bir DNN oluÅŸturmak iÃ§in, internetten kedi resimleri Ã¼zerinde Ã§alÄ±ÅŸmak ve 100 kedi resminizi test etmek, veri daÄŸÄ±lÄ±mlarÄ± farklÄ± olduÄŸundan iyi sonuÃ§lar vermeyebilir. Bu gibi durumlarda;\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1C5033OvW72TVhpUTGGKSjS3EoK80RNyS\" height=\"75px\" alt=\"split limited data sample figure\" />\n",
        "</center>\n",
        "\n",
        "100 kedi gÃ¶rÃ¼ntÃ¼sÃ¼ 50-50 olarak ayrÄ±labilir. (50) gÃ¶rÃ¼ntÃ¼ eÄŸitim iÃ§in internetteki gÃ¶rÃ¼ntÃ¼lerle karÄ±ÅŸtÄ±rÄ±labilir. \n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=15g9cYPzUlqdP-qcGtyYSPeykUX2raYEO\" alt=\"how to use the split limited data figure\" />\n",
        "</center>\n",
        "\n",
        "EÄŸitim ve doÄŸrulama/geÃ§erleme verileri farklÄ± daÄŸÄ±lÄ±mlardadÄ±r. Bu yÃ¼zden eÄŸitim ve doÄŸrulama hatalarÄ±nÄ±n karÅŸÄ±laÅŸtÄ±rÄ±lmasÄ± yÃ¼ksek varyans veya veri uyuÅŸmazlÄ±ÄŸÄ±na neden olur. Bu nedenle, eÄŸitim verileri, 50 kedi fotoÄŸrafÄ±nÄ±zÄ± karÄ±ÅŸtÄ±rdÄ±ktan sonra eÄŸitim ve eÄŸitim-doÄŸrulama olarak ayrÄ±lmÄ±ÅŸtÄ±r.\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1dYym9BHJm3T7Hb6NzdG9qKZq_pmx-E2l\" alt=\"final data after mixing all available data sources figure\" />\n",
        "</center>\n",
        "\n",
        "EÄŸitim ve eÄŸitim-doÄŸrulama setlerinin aynÄ± daÄŸÄ±lÄ±mdan geldiÄŸinde, problemin temel sebebinin bias veya varyans ya da veri uyuÅŸmazlÄ±ÄŸÄ± olduÄŸu anlaÅŸÄ±labilir.\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1e-UZ8fhPa-Vh1PXG0evre3apua2rfzkF\" alt=\"range of errors when data is from different distributions figure\" />\n",
        "</center>\n",
        "<center>\n",
        "  <caption>\n",
        "    <strong>Åekil 3: </strong>\n",
        "    TÃ¼m data aynÄ± daÄŸÄ±lÄ±mdan gelmediÄŸinde hata aralÄ±ÄŸÄ±\n",
        "  </caption>\n",
        "</center>\n",
        "\n",
        "Åekil 3'te gÃ¶sterildiÄŸi gibi, eÄŸitim-doÄŸrulama kÃ¼mesi ve doÄŸrulama kÃ¼mesi farklÄ± veri daÄŸÄ±lÄ±mlarÄ±ndan olduÄŸu iÃ§in, hatalarÄ± arasÄ±ndaki fark veri uyumsuzluÄŸundan kaynaklanmaktadÄ±r.\n"
      ]
    },
    {
      "metadata": {
        "id": "PiHZj81vFAGg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Regularizasyon / DÃ¼zenlileÅŸtime\n",
        "\n",
        "Sinir aÄŸÄ± aÅŸÄ±rÄ± uydurulduÄŸunda (yÃ¼ksek varyans) model eÄŸitim verisine uygun olduÄŸunda, gÃ¶rÃ¼nmeyen doÄŸrulama kÃ¼mesindeki tahminler kÃ¶tÃ¼ olabilir. DÃ¼zenlileÅŸtirme modeldeki (Ã§eÅŸitli/bazÄ±) nÃ¶ronlarÄ±n etkisini azaltÄ±r, bÃ¶ylece gÃ¶rÃ¼nmeyen girdilerin genelleÅŸtirilmesini saÄŸlar. Lambda $\\lambda$, L1 ve L2 algoritmalarÄ±nda kullanÄ±lan dÃ¼zenlileÅŸtirme miktarÄ±nÄ± kontrol eden hiperparametredir. DÃ¼zenleme iÃ§in bazÄ± algoritmalar / fikirler ÅŸÃ¶yledir:\n",
        "\n",
        "- L1 â€“ $W$â€™in ceza puanÄ±nÄ±n hesaplanmasÄ±nda L1-norm kullanÄ±lÄ±r\n",
        "- L2 â€“ $W$â€™in ceza puanÄ±nÄ±n hesaplanmasÄ±nda L2-norm kullanÄ±lÄ±r\n",
        "\n",
        "- Seyreltme (Dropout) â€“ aÄŸdaki bazÄ± nÃ¶ronlarÄ± rastgele olarak sÄ±fÄ±rlamak (yani silmek/seyreltme) modeli genelleÅŸtirmek iÃ§in basit bir yÃ¶ntemdir. $keep\\_prob$ bir nÃ¶ronun yeniden eÄŸitilme olasÄ±lÄ±ÄŸÄ±nÄ± hesaplayan bir hiperparametredir. FarklÄ± katmanlar, baÄŸlantÄ± yoÄŸunluÄŸuna baÄŸlÄ± olarak farklÄ± $keep\\_prob$ deÄŸerlerine sahip olabilir. \n",
        "\n",
        "- Veri artÄ±rma (Data augmentation) â€“ giriÅŸ/eÄŸitim gÃ¶rÃ¼ntÃ¼lerini (verilerini), rastgele kÄ±rpÄ±n, farklÄ± renk ve boyutlara dÃ¶nÃ¼ÅŸtÃ¼rÃ¼n\n",
        "\n",
        "- Erken durdurma (Early stopping) â€“ her epoch sonrasÄ±nda doÄŸrulama hatasÄ± artar ve eÄŸitim hatasÄ± azalmaya devam ederse (bu aÅŸÄ±rÄ± uydurma olduÄŸunun habercisi olabilir) erken durdurma uygulanmasÄ± Ã¶nerilir."
      ]
    },
    {
      "metadata": {
        "id": "4WC8LRtMFvqW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Normalizasyon ã€°ï¸\n",
        "\n",
        "Daha hÄ±zlÄ± Ã¶ÄŸrenme saÄŸlamak iÃ§in giriÅŸ Ã¶zelliklerini deÄŸiÅŸken aralÄ±klar ile normalleÅŸtirmek sÄ±kÃ§a baÅŸvurulan bir yÃ¶ntemdir. Normalizasyon, tÃ¼m eÄŸitim Ã¶rnekleri iÃ§in $\\mu=0$ ve $\\sigma^2=1$ deÄŸerini ayarlar.\n",
        "\n",
        "- KÃ¼me normalizasyon (Batch normalization) â€“ giriÅŸleri normalleÅŸtirme fikri tÃ¼m katmanlara geniÅŸletilir. Aktivasyon fonksiyonunu uygulamadan Ã¶nce $z^{[l]}$ normalleÅŸtirilir. Bu konÅŸulda parametreler ÅŸu ÅŸekilde hesaplanÄ±r:\n",
        "\n",
        "\n",
        "$$X \\xrightarrow{W^{[1]}, b^{[1]}} Z^{[1]} \\xrightarrow{\\beta^{[1]},\\gamma^{[1]}} \\tilde{Z}^{[1]} \\to a^{[1]} = g( \\tilde{Z}^{[1]}) \\xrightarrow{W^{[2]}, b^{[2]}} Z^{[2]}â€¦$$\n",
        "\n",
        "> $\\tilde{Z}^{[1]}$, $Z^{[1]}$'in normalize edilmiÅŸ halidir. $\\beta^{[1]}$ ve $\\gamma^{[1]}$ parametrelerinden faydalanarak hesaplanmaktadÄ±r.\n",
        "\n",
        ">TÄ±pkÄ± $W^{[l]}$ ve $b^{[l]}$ eÄŸitim sÄ±rasÄ±nda Ã¶ÄŸrenilen parametreler olduÄŸu gibi, $\\beta^{[l]}$ ve $\\gamma^{[l]}$ de aynÄ± ÅŸekilde Ã¶ÄŸrenilir.\n",
        "\n",
        "> Mini-kÃ¼me gradyan iniÅŸ ile, eÄŸitim sÄ±rasÄ±nda kÃ¼meler arasÄ± $\\mu$ ve $\\sigma^2$ Ã¼ssel aÄŸÄ±rlÄ±klÄ± ortalamalar hesaplanÄ±r ve kaydedilir. Bunlar, Ã§Ä±karÄ±m sÃ¼resinde $\\tilde{Z}^{[l]\\{t\\}}$ deÄŸerini hesaplamak iÃ§in kullanÄ±lÄ±r."
      ]
    },
    {
      "metadata": {
        "id": "mrc6yyA3J42n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Daha hÄ±zlÄ± ve iyi bir eÄŸitim\n",
        "\n",
        "- Mini-kÃ¼me gradyan iniÅŸ - eÄŸer eÄŸitim seti bÃ¼yÃ¼kse, modeller daha iyi Ã¶ÄŸrenir, ancak her epoch daha uzun sÃ¼rer. Mini-kÃ¼me gradyan iniÅŸinde, giriÅŸler kÃ¼meler halinde ayrÄ±lÄ±r ve bir mini-kÃ¼me Ã¼zerinde eÄŸitimden sonra bir gradyan iniÅŸ adÄ±mÄ± atÄ±lÄ±r. Mini-kÃ¼me boyutu, hem vektÃ¶rizasyon hem de daha hÄ±zlÄ± adÄ±mlardan yararlanmak iÃ§in genellikle 1 ila $m$ arasÄ±nda seÃ§ilir. Tipik kÃ¼me bÃ¼yÃ¼kleri 64, 128, 256 veya 512 eÄŸitim Ã¶rneÄŸidir, Ã¶yle ki her bir mini-kÃ¼me CPU / GPU hafÄ±zasÄ±na sÄ±ÄŸdÄ±rÄ±lmÄ±ÅŸ olur.\n",
        "\n",
        "- Momentum ile gradyan iniÅŸ - mini-kÃ¼me gradyan iniÅŸ, optimum seviyeye ulaÅŸmayÄ± yavaÅŸlatabilen osiloasyonlara/salÄ±nÄ±mlara neden olur. Momentum bu sorunu, hareketli/kayan ortalama yÃ¶ntemi ile etkileyici bir ÅŸekilde optimum deÄŸere daha hÄ±zlÄ± ulaÅŸmasÄ±nÄ± saÄŸlayarak Ã§Ã¶zer. Momentum $\\beta$, kayan pencere boyutu $\\approx \\frac{1}{1- Î²}$ ile kontrol edilir.\n",
        "\n",
        "- RMS Prop - Minimumdan uzaktayken daha uzun boyutlarda adÄ±mlar alarak ve minimuma yaklaÅŸtÄ±kÃ§a daha kÃ¼Ã§Ã¼k boyutlarda adÄ±mlar alarak gradyan iniÅŸ algoritmasÄ±nÄ± minimuma doÄŸru yÃ¶nlendirir. Bu Optimizasyon iÃ§in hiperparametreler $\\beta_2$ ve $\\epsilon$ olarak tanÄ±mlanÄ±r.  $\\epsilon$ Ã§ok Ã¶nemli deÄŸildir ve sadece sÄ±fÄ±r hatayla bÃ¶lÃ¼nmeyi Ã¶nlemek iÃ§in eklenir ve genellikle $10^{-8}$ olarak ayarlanÄ±r.\n",
        "\n",
        "- Adam â€“ gradyan iniÅŸ, momentum ve RMS Prop fikirlerinin birleÅŸiminden meydana gelen bir baÅŸka optimizasyon yÃ¶ntemidir. Hiperparametre olarak $\\beta$, $\\beta_2$  and $\\epsilon$ kullanÄ±lÄ±r.\n",
        "\n",
        "- Ã–ÄŸrenme hÄ±zÄ± azalmasÄ± - mini-kÃ¼me gradyan iniÅŸi minimum seviyeye osilasyon/salÄ±nÄ±mlar ekler. Ã–ÄŸrenme hÄ±zÄ±na bir bozulma katsayÄ±sÄ± eklemek daha iyi yakÄ±nsamasÄ±nÄ± saÄŸlar. BÃ¶ylece $\\alpha$ artÄ±k bir sabit deÄŸildir ve ÅŸÃ¶yle hesaplanÄ±r:\n",
        "\n",
        "$$\\alpha=\\frac{1}{(1 + bozulma\\_oranÄ± \\times epoch\\_sayÄ±sÄ±) \\times \\alpha_0}$$\n"
      ]
    },
    {
      "metadata": {
        "id": "Z5OheTD8L82x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Hiperparametrelerin ayarlanmasÄ± ğŸ”§ ğŸ”¨\n",
        "\n",
        "EÄŸitimden Ã¶nce ayarlanan birÃ§ok hiperparametre olduÄŸundan, hepsinin eÅŸit derecede Ã¶nemli olmadÄ±ÄŸÄ±nÄ± anlamak Ã¶nemlidir. Ã–rneÄŸin,  $\\alpha$, $\\lambda$'dan daha Ã¶nemlidir, bu nedenle Ã¶nce $\\alpha$ ayarlamasÄ±nÄ±n yapÄ±lmasÄ± daha iyidir. Hiperparametreyi ayarlamak iÃ§in bazÄ± yaklaÅŸÄ±mlar ÅŸÃ¶yledir:\n",
        "\n",
        "- Grid/Izgara tabanlÄ± arama - Hiperparametre 1 ve 2 deÄŸerlerinin kombinasyonlarÄ±nÄ± iÃ§eren bir tablo oluÅŸturun. Her kombinasyon iÃ§in, en iyi kombinasyonu bulmak Ã¼zere doÄŸrulama kÃ¼mesinde deÄŸerlendirin.\n",
        "\n",
        "- Rastgele tabanlÄ± arama - 1 ve 2 hiperparametre deÄŸerleri iÃ§in rastgele kombinasyonlar seÃ§in. Her bir kombinasyonda, en iyi kombinasyonu bulmak Ã¼zere doÄŸrulama kÃ¼mesini deÄŸerlendirin. GeniÅŸ bir deÄŸer alanÄ±nda rastgele bir arama yaptÄ±ktan sonra, kaba rasgele aramanÄ±n sonuÃ§larÄ±nÄ± kullanarak ilgilenilen alanda daha ince ve detaylÄ± bir arama yapÄ±labilir. Rasgele eÅŸik deÄŸerleri seÃ§meden Ã¶nce hiperparametrelerin Ã¶lÃ§eklendirilmesi Ã¶nemlidir.\n",
        "\n",
        "- Panda VS Caviar yaklaÅŸÄ±mÄ± - EÄŸer model Ã§oklu kombinasyonlarÄ±n test edilemeyeceÄŸi kadar karmaÅŸÄ±ksa, en iyi fikir $J$'nin zamanla nasÄ±l deÄŸiÅŸtiÄŸini ve Ã§alÄ±ÅŸma sÃ¼resinde hiperparametre deÄŸerlerini nasÄ±l deÄŸiÅŸtirdiÄŸini izleyerek en iyi deÄŸeri bulmaktÄ±r.\n"
      ]
    },
    {
      "metadata": {
        "id": "RAGhT4dLMgKo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Ã‡oklu SÄ±nÄ±flandÄ±rma ğŸ ğŸ\n",
        "\n",
        "\n",
        "Softmax katmanÄ±, $C$ sayÄ±da sÄ±nÄ±fÄ± sÄ±nÄ±flandÄ±rmak iÃ§in son katman olarak kullanÄ±lÄ±r. Son katman olan $L$'den gelen aktivasyonlar ÅŸu ÅŸekilde hesaplanÄ±r:\n",
        "\n",
        "$t_i = (e^{z_i})^{[L]}$ deÄŸerleri iÃ§in\n",
        "$$a_i^{[L]} = \\frac{t_i}{\\sum_{j=1}^C t_i}$$"
      ]
    },
    {
      "metadata": {
        "id": "eC3vGlRyNkNl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Transfer Ã–ÄŸrenme ğŸ™‡\n",
        "\n",
        "Bir modelde Ã¶ÄŸrenilen parametreleri bir baÅŸka model iÃ§in kullanÄ±lmasÄ±dÄ±r. Orijinal eÄŸitimli aÄŸdaki son birkaÃ§ katmanÄ± deÄŸiÅŸtirerek yeni problem iÃ§in kullanÄ±lmasÄ±dÄ±r. Yeni katmanlar daha sonra ilgilenilen yeni veri kÃ¼mesi kullanÄ±larak eÄŸitilebilir. Bu, genellikle mevcut bir modelin ilk katmanlarÄ± tarafÄ±ndan tanÄ±mlanan Ã¶zellikler baÅŸka bir gÃ¶rev iÃ§in yeniden kullanÄ±labildiÄŸinde uygulanabilir. BÃ¶ylece iÅŸlem yÃ¼kÃ¼ konusunda da optimizasyon yapÄ±lmÄ±ÅŸ olur."
      ]
    },
    {
      "metadata": {
        "id": "CTtgXHRKNxPT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## EvriÅŸimli Sinir AÄŸlarÄ± (Convolutional Neural Networks - CNN) ğŸ“¸ ğŸ¥\n",
        "\n",
        "EvriÅŸimli sinir aÄŸlarÄ±, derin sinir aÄŸlarÄ±nÄ±n bilgisayarlÄ± gÃ¶rme konusunda spesifik olarak kullanÄ±lan bir alt konusudur. Bir derin sinir aÄŸÄ±nÄ±n, $X$'in Ã¶zelliklerini elle ayarlamaya gerek kalmadan (Ã¶znitelik Ã§Ä±karma - feature extraction) tanÄ±mlamasÄ± beklenir. Bu nedenle, bilgisayarlÄ± gÃ¶rÃ¼ gÃ¶revlerinde $X$ olarak genellikle gÃ¶rÃ¼ntÃ¼ler, videolar kullanÄ±lÄ±r. Ã–znitelik  mÃ¼hendisliÄŸi yapÄ±lmadan, gÃ¶rÃ¼ntÃ¼ aÄŸa olduÄŸu gibi iletilirse, Ã¶ÄŸrenilecek parametre sayÄ±sÄ± gÃ¶rÃ¼ntÃ¼nÃ¼n Ã§Ã¶zÃ¼nÃ¼rlÃ¼ÄŸÃ¼ne baÄŸlÄ± olarak oldukÃ§a yÃ¼ksek olabilir. Ã–rneÄŸin, giriÅŸ gÃ¶rÃ¼ntÃ¼sÃ¼ (geniÅŸlik, yÃ¼kseklik, RGB kanallarÄ±) = (1000, 1000, 3) boyutlu ise, tam olarak (Åekil 1'de gÃ¶sterildiÄŸi gibi)  $n^{[1]} = 1000$ olan bir katmana baÄŸlanmasÄ±, $W$ anlamÄ±na gelir. $W.shape = (1000, 3\\times10^6)$, yani 3 milyar parametre hesaplanacaÄŸÄ± anlamÄ±na gelir. Bu kadar Ã§ok parametrenin eÄŸitimi, Ã§ok sayÄ±da eÄŸitim verisi gerektirir ve bu nedenle derin sinir aÄŸÄ±ndan gelen mevcut Ã¶ngÃ¶rÃ¼ler bilgisayarla gÃ¶rme uygulamalarÄ± iÃ§in kullanÄ±lmaz. Bu nedenle, EvriÅŸimli Sinir AÄŸlarÄ± (CNN) adÄ± verilen yeni bir sÄ±nÄ±f geliÅŸtirilmiÅŸtir.\n",
        "\n",
        "Bir derin sinir aÄŸÄ±nda daha Ã¶nceki katmanlarÄ±n kenarlar gibi basit Ã¶znitelikleri/Ã¶zellikleri tanÄ±mladÄ±ÄŸÄ± ve daha sonrakilerinin belirli bir gÃ¶rÃ¼ntÃ¼de daha karmaÅŸÄ±k Ã¶znitelikleri ve Ã¶rÃ¼ntÃ¼leri tespit ettiÄŸi bilinmektedir. Matematikteki evriÅŸim $\\ast$ operatÃ¶rÃ¼, yukarÄ±daki problemleri Ã§Ã¶zer - hem daha Ã¶nceki katmanlardaki kenarlarÄ± tanÄ±mlar hem de tamamen baÄŸlÄ± bir derin sinir aÄŸÄ±ndan daha az parametre gerektirir.\n",
        "\n",
        "\n",
        "---\n",
        "### AdÄ±m adÄ±m bir evriÅŸimli sinir aÄŸÄ± tasarlamak isterseniz baÄŸlantÄ±yÄ± ziyaret ediniz! ğŸ’»\n",
        "* **[AdÄ±m AdÄ±m EvriÅŸimli Sinir AÄŸÄ± Modeli OluÅŸturma](https://github.com/ayyucekizrak/Udemy_DerinOgrenmeyeGiris/blob/master/Evrisimli_Sinir_Aglari/EvrisimliSinirAgi_AdimAdim.ipynb)**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "D7bmAD4VPT5s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### EvriÅŸim iÅŸlemi nasÄ±l yapÄ±lÄ±r?\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1Lp2Flx0Ad--sTRzWAtVmQEIDaaEscP95\" alt=\"convolution operator animation\" />\n",
        "</center>\n",
        "<center>\n",
        "  <caption>\n",
        "    <strong>Åekil 4:</strong> EvriÅŸim iÅŸlemi\n",
        "  </caption>\n",
        "  <p>\n",
        "    <small>\n",
        "      <strong>Source:</strong> Kursun iÃ§indeki kodlama Ã¶rneÄŸi â€œConvolution model - Step by Step - v2 (AdÄ±m adÄ±m evriÅŸim modeli)â€ https://www.coursera.org/learn/convolutional-neural-networks/\n",
        "    </small>\n",
        "  </p>\n",
        "</center>\n",
        "\n",
        "- GiriÅŸ katmanÄ±ndaki kanallarÄ±n sayÄ±sÄ± (3. boyut) evriÅŸim filtresindeki boyut sayÄ±sÄ±yla eÅŸleÅŸmelidir.\n"
      ]
    },
    {
      "metadata": {
        "id": "BIDNJPYERINP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Doldurma / Piksel Ekleme\n",
        "\n",
        "$\\ast$  Ã§alÄ±ÅŸma ÅŸekli nedeniyle, keanr bÃ¶lgelerdeki matris deÄŸerlerinin (pikseller olabilir) hesaplamaya katkÄ±sÄ± daha ortalardaki deÄŸerlerden daha azdÄ±r. Bu problemi Ã§Ã¶zmek iÃ§in dÄ±ÅŸ kenarlara deÄŸerler eklenir. Buna doldurma ya da piksel ekleme denir $p$ ile gÃ¶sterilir. Ä°ki ÅŸekilde yapÄ±labilir. Ya komÅŸusu olan deÄŸer devamÄ±na eklenir ya da 0 deÄŸerleri ile doldurulabilir. \n",
        "\n",
        "-  SÄ±fÄ±r (Same) $\\implies p = 0$\n",
        "\n",
        "- Var olan ile (Valid) $f$ evriÅŸim yapÄ±lan filtrenin boyutu iÃ§in $p = \\frac{f-1}{2}$ \n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1tyfeyh1ewNknatMgGS1q-aVkpXQ9WXH4\" alt=\"padding\" />\n",
        "</center>\n",
        "<center>\n",
        "  <caption>\n",
        "    <strong>Åekil 5:</strong> Doldurma / Piksel Ekleme\n",
        "  </caption>\n",
        "  <p>\n",
        "    <small>\n"
      ]
    },
    {
      "metadata": {
        "id": "R67_XjEOR9ah",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Bir evriÅŸim iÅŸlemindeki boyut hesaplarÄ±\n",
        "\n",
        "$$(n, n, \\#kanal sayÄ±sÄ±) \\ast (f, f, \\#kanal sayÄ±sÄ±) \\to (\\lfloor \\frac{n + 2p - f}{s + 1} \\rfloor,\\lfloor \\frac{n + 2p - f}{s + 1} \\rfloor, \\#filtre sayÄ±sÄ±)$$\n",
        "\n",
        "$n$ = giriÅŸ katmanÄ± boyutu / gÃ¶rÃ¼ntÃ¼ iÃ§in\n",
        "\n",
        "> $f$ = evriÅŸim filtresi boyutu\n",
        "\n",
        "> $p$ = giriÅŸ katmanÄ±na yapÄ±lan ekleme miktarÄ±\n",
        "\n",
        "> $s$ = giriÅŸ katmanÄ±nda evriÅŸimiÅŸlemi yapÄ±lÄ±rken iÅŸlemler arasÄ± adÄ±m kaydÄ±rma sayÄ±sÄ±\n",
        "\n",
        "> $\\# filters$ = giriÅŸ katmanÄ±nda evriÅŸim iÃ§in kullanÄ±lan filtre sayÄ±sÄ±\n",
        "\n",
        "Åekil 4 iÃ§in: $n = 5, \\# kanal sayÄ±sÄ± = 1, f = 3, p = 0, s = 1, \\# filtre sayÄ±sÄ± = 1$\n"
      ]
    },
    {
      "metadata": {
        "id": "sbYYOyJfVglB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Ortaklama\n",
        "\n",
        "GiriÅŸin yÃ¼ksekliÄŸini ve geniÅŸliÄŸini kÃ¼Ã§Ã¼ltmek iÃ§in kullanÄ±lan, $\\ast$ gibi baÅŸka bir iÅŸlem tÃ¼rÃ¼dÃ¼r. AynÄ± $\\ast$ gibi, ortaklama katmanlarÄ± da girdi boyunca uzanan filtrelerdir. Ancak, Ã¶ÄŸrenecek herhangi bir parametresi yoktur.\n",
        "\n",
        "- Maksimum Ortaklama - GiriÅŸe uygulanan filtrenin her konumunda maksimum deÄŸeri seÃ§ilir ve dÄ±ÅŸarÄ± aktarÄ±lÄ±r.\n",
        "\n",
        "- Ortalama Ortaklama - GiriÅŸe uygulanan filtrenin her konumundaki ortalama deÄŸer hesaplanÄ±r ve dÄ±ÅŸarÄ± aktarÄ±lÄ±r.\n",
        "https://drive.google.com/open?id=\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1pl7drPDom0YtwyPGIK-WGdtVhTbL8YTv\" alt=\"max pooling\" />\n",
        "</center>\n",
        "<center>\n",
        "  <caption>\n",
        "    <strong>Ortalama Ortaklama</strong> \n",
        "  </caption>\n",
        " <center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1yI58nXx5BJboRNuwdNt2D2qpwEDiVFB4\" alt=\"avgerage pooling\" />\n",
        "</center>\n",
        "<center>\n",
        "  <caption>\n",
        "    <strong>Maksimum Ortaklama</strong> \n",
        "  </caption>\n",
        "  \n",
        "    \n",
        "  \n"
      ]
    },
    {
      "metadata": {
        "id": "ZyJQQ50JcETB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## SÄ±ralÄ± / Dizi Modeller ğŸ—£\n",
        "\n",
        "Dizi veya dizi hale dÃ¶nÄŸÅŸtÃ¼rÃ¼lebilen girdilerin modellenmesi iÃ§in kullanÄ±lan bir baÅŸka derin sinir aÄŸÄ± alt konusudur. Ã–rneÄŸin, bir kelime dizisi bir cÃ¼mle, hava basÄ±ncÄ± deÄŸerleri dizisi (zamanla) bir ses / mÃ¼zik vb. bunlar dizi tipi verilerdir ve bu tip modellerle iÅŸlenmektedir.\n",
        "\n",
        "### DoÄŸal Dil Ä°ÅŸleme (Natural Language Processing - NLP ğŸ’¬ ğŸ’­\n",
        "\n",
        "Dizi modelleme teknikleri, DoÄŸal (Ä°nsan) dili iÅŸlemek iÃ§in yaygÄ±n olarak kullanÄ±lmaktadÄ±r. Sinir aÄŸlarÄ± sayÄ± matrisleri Ã¼zerinde Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan (Ä°ngilizce) kelimeleri sayÄ±lara kodlamanÄ±n basit bir yolu onlarÄ± one-hot encode kodlamaktÄ±r. Bunu yapmak iÃ§in, Ä°ngilizce sÃ¶zlÃ¼kte 10000 kelime olduÄŸunu varsayarsak, her kelimeye (10000, 1) arasÄ±nda benzersiz bir rasgele sayÄ± atanÄ±r. O zaman, eÄŸer 'Aaron' kelimesi 3 numaraya atanmÄ±ÅŸsa, one-hot kodlanmÄ±ÅŸ matrisi,\n",
        "\n",
        "<center>\n",
        "  $\n",
        "  \\begin{bmatrix}\n",
        "    0 \\\\\n",
        "    0 \\\\\n",
        "    1 \\\\\n",
        "    0 \\\\\n",
        "    . \\\\\n",
        "    . \\\\\n",
        "    . \\\\\n",
        "    0 \\\\\n",
        "  \\end{bmatrix}_{10000\\times 1}\n",
        "  $\n",
        "</center>\n",
        "\n",
        "\n",
        "Her sÃ¶zcÃ¼k one-hot temsiline dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼kten sonra, cÃ¼mle, Åekil 6 ve 7'de gÃ¶sterildiÄŸi gibi bir Yineleyen (Ã–zyineli) Sinir AÄŸÄ±na (Recurrent Neural Networks - RNN) verilir. Verilen bir cÃ¼mle iÃ§in, \"Aaron her gÃ¼n okula bisikletle gider\", $x^{<1>}$  'Aaron' un one-hot kodlamasÄ±,  'her gÃ¼n'Ã¼n one-hot kodlamasÄ± $x^{<2>}$ olur. BÃ¶yle devam eder tÃ¼m cÃ¼mle iÃ§in.  EÄŸer RNN'nin amacÄ± cÃ¼mle iÃ§indeki her kelimenin konuÅŸma bÃ¶lÃ¼mlerini bulmaksa, o zaman $y^{<1>}$ 'Aaron' iÃ§in konuÅŸmanÄ±n bir parÃ§asÄ± olacaktÄ±r, $y^{<2>}$ 'her gÃ¼n' iÃ§in konuÅŸmanÄ±n bir parÃ§asÄ± olacaktÄ±r. Bu gÃ¶revin eÄŸitim verileri, X = ingilizce cÃ¼mleler iÃ§indeki one-hot kodlanmÄ±ÅŸ ingilizce kelimeler ve Y = her cÃ¼mle iÃ§in her kelimenin konuÅŸma iÃ§indeki bÃ¶lÃ¼mleri olacaktÄ±r.\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1DO4NiO_MnorVlCqX_nUdjsoRNCNbnGbc\" alt=\"a recurrent neural network\" />\n",
        "</center>\n",
        "<center>\n",
        "  <caption>\n",
        "    <strong>Åekil 6:</strong> Dizi / SÄ±ralÄ± Model\n",
        "  </caption>\n",
        "</center>\n",
        "\n",
        "---\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1QTnfyewDMEm6HhTLNgzTe1nJhVTKC9KU\" alt=\"a recurrent neural network\" />\n",
        "</center>\n",
        "<center>\n",
        "  <caption>\n",
        "    <strong>Åekil 7:</strong> Yinelenen (Ã–zyinelemeli) Sinir AÄŸÄ±\n",
        "  </caption>\n",
        "</center>\n",
        "\n",
        "### Neden sÄ±radan bir derin sinir aÄŸÄ± kullanmak yerine RNN kullanmalÄ±yÄ±z?\n",
        "\n",
        "CNN'lerin bilgisayar gÃ¶rme gÃ¶revleri iÃ§in uygun olduÄŸu gibi, RNN'ler de zamanla deÄŸiÅŸme Ã¶zelliÄŸi olan yapÄ±ya sahip gÃ¶revler iÃ§in tasarlanmÄ±ÅŸtÄ±r.\n",
        "\n",
        "* Ã‡Ä±ktÄ±larÄ±n farklÄ± uzunluklarda olmasÄ±na izin verin $T_X \\neq T_Y$, Ã¶rneÄŸin Makine Ã‡evirisi (Machine Translation) gÃ¶revinde Ã§Ä±ktÄ± cÃ¼mlesinin uzunluÄŸunun giriÅŸ cÃ¼mlesine baÄŸlÄ± olmaktadÄ±r.\n",
        "* FarklÄ± metin konumlarÄ±nda Ã¶ÄŸrenilen Ã¶znitelikleri/Ã¶zellikleri paylaÅŸabilir (NLP'de)\n",
        "* RNN'ler de, CNN'lerde olduÄŸu gibi DNN'lere kÄ±yasla hesaplamak iÃ§in daha az parametreye sahiptir.\n",
        "\n",
        "[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)  Andrej Karpathyblog yazÄ±sÄ± farklÄ± tiplerdeki RNN ve NLP modellerinden bahsetmektedir. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "pnQIrImRM5Xn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### RNN'lerde Ä°leri YayÄ±lÄ±m â¡ï¸\n",
        "\n",
        "* Genellikle: $a^{<0>} = \\vec{0}$\n",
        "* $W_{aa}, W_{ax}, W_{ya}, b_a$ and $b_y$ gradyan iniÅŸ algoritmasÄ± ile Ã¶ÄŸrenilen parametreler \n",
        "* $g_1$ genellikle tanh ya da ReLU ve $g_2$ genellikle sigmoid ya da softmax\n",
        "\n",
        "iÃ§in;\n",
        "\n",
        "<center>\n",
        "  <p>$a^{<t>} = g_1(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a)$</p>\n",
        "  <p>$\\hat{y}^{<t>} = g_2(W_{ya}a^{<t>} + b_y)$</p>\n",
        "</center>\n",
        "\n",
        "Parametrelerin &lt;t&gt; zaman adÄ±mlarÄ±nda paylaÅŸÄ±ldÄ±ÄŸÄ±nÄ± unutmayÄ±n. GÃ¶sterim ÅŸuÅŸekilde basitleÅŸtirilebilir:\n",
        "\n",
        "* $W_a$, $W_{aa}$ ve $W_{ax}$ matrislerinin sÃ¼tunudur. \n",
        "* $[a^{<t-1>}, x^{<t>}]$ sÄ±rasÄ±yla satÄ±rlar birleÅŸtirilir\n",
        "\n",
        "bu durumda: \n",
        "\n",
        "<center>\n",
        "$\n",
        "  \\begin{bmatrix}\n",
        "      W_{aa} & W_{ax}\n",
        "  \\end{bmatrix}\n",
        "  \\begin{bmatrix}\n",
        "      a^{<t-1>} \\\\\n",
        "      x^{<t>}\n",
        "  \\end{bmatrix} = W_{aa}a^{<t-1>} + W_{ax}x^{<t>}\n",
        "$\n",
        "</center>\n",
        "\n",
        "<center>\n",
        "  <p>$a^{<t>} = g(W_a[a^{<t-1>}, x^{<t>}] + b_a)$</p>\n",
        "  <p>$\\hat{y}^{<t>} = g(W_ya^{<t>} + b_y)$</p>\n",
        "</center>\n",
        "\n",
        "\n",
        "### RNN'lerde Geriye YayÄ±lÄ±m â†©ï¸ \n",
        "\n",
        "Åekil 8'de, parametrelerin ileri yayÄ±lÄ±m adÄ±mÄ±ndaki kaybÄ± hesaplamak iÃ§in nasÄ±l ileri doÄŸru iÅŸlemlendiÄŸi ve gradyan iniÅŸinin ve geriye yayÄ±lÄ±m yoluyla parametreleri ayarlamak iÃ§in kayÄ±p/yitim $L$ tÃ¼revlerini nasÄ±l hesaplanarak gÃ¼ncellendiÄŸi gÃ¶rÃ¼lebilir.\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1S7febiHMC1utTh1pPSTfKn7MsTDiAluy\" alt=\"recurrent neural network computational graph\" />\n",
        "</center>\n",
        "<center>\n",
        "  <caption>\n",
        "    <strong>Åekil 8:</strong> Tekrarlayan/ Ã–zyinelemeli Sinir AÄŸÄ± Hesaplama GrafiÄŸi  </caption>\n",
        "</center>\n",
        "\n",
        "<br />\n",
        "\n",
        "Denklemler incelendiÄŸinde, ilk olarak Ã¶nceki $L^{<t>}$ olduÄŸu gibi Ã§apraz entropi kaybÄ± kullanÄ±larak hesaplanÄ±r. Nihai kayÄ±p $L$L, her $t$ zamanÄ± iÃ§in hesaplanan kayÄ±plarÄ±n toplamÄ±dÄ±r.\n",
        "\n",
        "<center>\n",
        "  <p>$L^{<t>}(\\hat{y}^{<t>}, y^{<t>}) = - (y^{<t>}log(\\hat{y}^{<t>}) - (1-y^{<t>})log(1-\\hat{y}^{<t>}))$</p>\n",
        "  <p>$L(\\hat{y}, y) = \\sum_{t=1}^{T_y} L^{<t>}(\\hat{y}^{<t>}, y^{<t>})$</p>\n",
        "</center>\n",
        "\n",
        "Ã–zyinelemeli sinir aÄŸlarÄ± $a$ katmanlarÄ±Ä±ndaki aktivasyon fonksiyonlarÄ± ve zamana baÄŸlÄ± iÅŸlemlerin yapÄ±lÄ±ÅŸ ÅŸekline gÃ¶re sÄ±nÄ±flandÄ±rÄ±labilir. Bu iÃ§ birimlerdeki baÄŸlantÄ± ve aktivasyon iÅŸlemleri modelin bellek Ã¶zelliklerini doÄŸrudan etkilemektedir. HatÄ±rlama ve unutma gibi iÅŸlemler bu birimlerde gerÃ§ekleÅŸtirilir.\n"
      ]
    },
    {
      "metadata": {
        "id": "374uU6b4_zLs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Dil Modeli\n",
        "\n",
        "Dilbilgisel ve mantÄ±ksal olarak, \"Ram ate an apple\", \"Ram <b>an ate</b> apple\" den daha muhtemeldir. Dil modelinin amacÄ±, ilk cÃ¼mleye daha yÃ¼ksek bir olasÄ±lÄ±k atamaktÄ±r. BaÅŸka bir deyiÅŸle, dil modeli\n",
        "\n",
        "<center>\n",
        "  $P(y^{<1>} = Ram,\\ y^{<2>} = ate,\\ y^{<3>} = an,\\ y^{<4>} = apple)\\ \\mathbf{>}\\ P(y^{<1>} = Ram,\\ y^{<2>} = an,\\ y^{<3>} = ate,\\ y^{<4>} = apple)$\n",
        "</center>\n",
        "\n",
        "Bu gÃ¶rev iÃ§in bire Ã§ok (one-to-more) RNN kullanÄ±lÄ±r. $y^{<t>}$ kelime iÃ§indeki tÃ¼m kelimelerin cÃ¼mle iÃ§inde $<t>$ konumunda bulunma olasÄ±lÄ±klarÄ±dÄ±r. $y^{<t-1>}$ girdi olarak beslenir, $x^{<t>}$ koÅŸullu bir olasÄ±lÄ±k oluÅŸturur $P(y^{<2>} = ate\\ |\\ y^{<1>} = Ram)$\n",
        "  \n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=181G51N_MwiB-dz9ufBsXOSrmcALILz_o\" alt=\"one-to-many RNN for modelling language\" />\n",
        "</center>\n",
        "<center>\n",
        "  <caption>\n",
        "    <strong>Åekil 9:</strong> Dil modeli iÃ§in bire Ã§ok (one-to-many) RNN \n",
        "  </caption>\n",
        "</center>\n",
        "\n",
        "Bunu bÃ¼yÃ¼k bir metin verisi Ã¼zerinde eÄŸitmek, bir dile Ã¶zgÃ¼ kelimelerin sÄ±rasÄ±nÄ± modeller. Her bir $<t>$ 'de elde edilen koÅŸullu olasÄ±lÄ±klarÄ± kullanarak, bir cÃ¼mlenin olasÄ±lÄ±ÄŸÄ±nÄ± bulabilir,\n",
        "\n",
        "<center>\n",
        "  $P(y^{<1>} = Ram,\\ y^{<2>} = ate,\\ y^{<3>} = an,\\ y^{<4>} = apple) = P(y^{<1>} = Ram) \\times P(y^{<2>} = ate\\ |\\ y^{<1>} = Ram) \\times P(y^{<3>} = an\\ |\\ y^{<1>} = Ram, \\ y^{<2>} = ate) \\times P(y^{<4>} = apple\\ |\\ y^{<1>} = Ram, \\ y^{<2>} = ate, \\ y^{<3>} = an)$\n",
        "</center>\n",
        "\n",
        "<br />\n",
        "\n",
        "$P(y^{<3>} = an\\ |\\ y^{<1>} = Ram, \\ y^{<2>} = ate)$,\n",
        "\n",
        "$3.$ nÃ¶rondan elde edilir. &lt;UNK> sÃ¶zlÃ¼kte bulunmayan kelimeler iÃ§in kullanÄ±lan Ã¶zel bir belirteÃ§tir. \n",
        "\n",
        "Benzer ÅŸekilde karakter seviyeli dil modelleri her karakterin $\\hat{y}^{<t>}$ ÅŸekilde ifade edilmesi ve &lt;UNK> olarak tanÄ±mlanma dil modelinin baÅŸarÄ±mÄ±na az da olsa katkÄ± saÄŸlamaktadÄ±r. Ancak karakter modelleriyle ilgili sorun, dizilerin Ã§ok daha uzun olacaÄŸÄ± ve RNN'lerin Ã§ok uzun menzilli baÄŸÄ±mlÄ±lÄ±klarÄ± taÅŸÄ±yamayacaÄŸÄ±dÄ±r. AyrÄ±ca, karakter modelleri hesaplama yoÄŸundur ve Ã§ok yaygÄ±n deÄŸildir.\n"
      ]
    },
    {
      "metadata": {
        "id": "Mb_eN0vfD8gg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### GeÃ§itlenmiÅŸ Ã–zyinelemeli Birimler (Gated Recurrent Units - GRU)\n",
        "\n",
        "GradyanlarÄ±n yok olmasÄ± problemi hemen her derin sinir aÄŸÄ± modelinde olduÄŸu gibi RNN'lerde de vardÄ±r. Uzun vadeli baÄŸÄ±mlÄ±lÄ±klar Ä°ngilizcede olduÄŸu gibi bir Ã§ok dilde de yaygÄ±ndÄ±r. CÃ¼mlenin baÅŸÄ±nda kullanÄ±lan bir kelime sonundaki bir durumu belirleyebilir. Uzun vadeli baÄŸÄ±mlÄ±lÄ±klarÄ± saÄŸlamak iÃ§in GeÃ§itlenmiÅŸ Ã–zyinelemeli Birimler bellek hÃ¼cresi adÄ± verilen bir konsept sunar. \n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=18aO-ZOV9C8qfymZbcHiCHcycsNZZxMb5\" alt=\"RNN and GRU cells\" />\n",
        "</center>\n",
        "<center>\n",
        "  <caption>\n",
        "    <strong>Åekil 10:</strong> RNN ve GRU hÃ¼creleri\n",
        "  </caption>\n",
        "</center>\n",
        "\n",
        "<br />\n",
        "<center>\n",
        "  <p>$\\tilde{c}^{<t>} = tanh(W_c[c^{<t-1>},\\ x^{<t>}] + b_c) $</p>\n",
        "  <p>$\\Gamma_u = \\sigma(W_u[c^{<t-1>},\\ x^{<t>}] + b_u) $</p>\n",
        "</center>\n"
      ]
    },
    {
      "metadata": {
        "id": "Fl94O2ATF28S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Uzun-KÄ±sa Vadeli Bellekler (Long-Short Term Memory - LSTMs)\n",
        "\n",
        "LSTM, GRU yapÄ±sÄ±nÄ±n Ã¶zelleÅŸtirilmiÅŸ bir hali olarak tanÄ±mlanabilir. AynÄ± zamanda GRU LSTM'e gÃ¶re daha kolay hesaplanabilirdir. LSTM yapÄ±sÄ±nda geÃ§miÅŸ ve gelecekteki bilgiyi anlamlandÄ±racak Ã¶znitelikler Ã¶zyinelemeli olarak taÅŸÄ±nmaktadÄ±r. Åekil 11'de basit bir LSTM yapÄ±sÄ± gÃ¶sterilmektedir. Bu modelde Ã¼Ã§ farklÄ± noktada (giriÅŸ, hatÄ±rlama/unutma, Ã§Ä±kÄ±ÅŸ) aktivasyon fonksiyonu kullanÄ±lmaktadÄ±r. GiriÅŸ ve Ã§Ä±kÄ±ÅŸ katmanlarÄ±nda genellikle $tanh$ fonksiyonu, hatÄ±rlama/unutma kapÄ±larÄ±nda ise her zaman $sigmoid$ fonksiyonu kullanÄ±lmaktadÄ±r. Bu yapÄ±nÄ±n GRUâ€™dan en Ã¶nemli farkÄ±, ilgililik geÃ§itinin LSTM yapÄ±sÄ±nda Ã¶zelleÅŸerek unutma $\\Gamma_f $ ve Ã§Ä±kÄ±ÅŸ geÃ§iti $\\Gamma_o $ olarak iki yeni denklem ile elde ediliyor olmasÄ±dÄ±r. Unutma geÃ§iti sayesinde geÃ§miÅŸten aktarÄ±lan ancak gerekli olmayan bilgilerin aÄŸÄ±rlÄ±klarÄ±nÄ±n azaltÄ±lmasÄ± saÄŸlanmaktadÄ±r. GÃ¼ncelleme geÃ§iti ile unutmanÄ±n da etkisi ile daha efektif bir Ã§Ä±kÄ±ÅŸ Ã¼retilmektedir.\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1AQb1bvZ1eFFDKTQ5Eh0-WViSt9RH8p8C\" alt=\"RNN and GRU cells\" />\n",
        "</center>\n",
        "<center>\n",
        "  <caption>\n",
        "    <strong>Åekil 11:</strong> LSTM yapÄ±sÄ±\n",
        "  </caption>\n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "MTDsAEpJWEMB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Derin Ã–ÄŸrenme Hiperparametreleri \n",
        "\n",
        "Hiperparametre | Sembol | Ortak DeÄŸerler\t|\n",
        "--- | --- | --- | ---\n",
        "regularizasyon | $\\lambda$ | | \"aÄŸÄ±rlÄ±k azaltÄ±lmasÄ±\" ya da \"dÃ¼zenlileÅŸtirme\" olarak ta bilinir\n",
        "Ã¶ÄŸrenme oranÄ± | $\\alpha$\t| 0.01 | \n",
        "keep_prob | | 0.7 | Seyreltme (Dropout) iÅŸleminde kullanÄ±lÄ±r\n",
        "momentum | $\\beta$ | 0.9 | Adam optimizasyonunda da kullanÄ±lÄ±r \n",
        "mini-kÃ¼me boyutu |  $t$\t| 64, 128, 256, 512\t| \n",
        "RMS Prop | $\\beta_2$ | 0.999 | Adam optimizasyonunda da kullanÄ±lÄ±r  \n",
        "Ã¶ÄŸrenme oranÄ± azaltÄ±lmasÄ± | | | azaltma oranÄ± (decay_rate) olarak da bilinir\n",
        "filtre boyutu | $f^{[l]}$\t|\t| CNN'de, $l$ katmanÄ±ndaki biltrenin boyutu\n",
        "adÄ±m aralÄ±ÄŸÄ± (stride) | $s^{[l]}$ | | CNN'de, $l$ katmanÄ±ndaki adÄ±m aralÄ±ÄŸÄ± deÄŸeri\n",
        "doldurma/piksel ekleme (padding) |  $p^{[l]}$ | | CNN'de, $l$ katmanÄ±ndaki doldurma/ekleme deÄŸeri\n",
        "\\# filtre sayÄ±sÄ± | $n_c^{[l]}$ | | CNN'de, $l$ katmanÄ±nda kullanÄ±lan filtre sayÄ±sÄ± \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\t\t\t\n",
        "\t\t\t\n"
      ]
    },
    {
      "metadata": {
        "id": "5JI8wnui8Pwo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### âš¡ï¸ SorularÄ±nÄ±z ve notlarÄ±n daha fazlasÄ±na eriÅŸmek iÃ§in [Deep Learning TÃ¼rkiye](http://deeplearningturkiye.com/) kaynaklarÄ±ndan faydalanabilirsiniz! âš¡ï¸\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Gz4dM6CB9RQ7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### ğŸŠDeep Learning (Deep Neural Networks) Video Ders Kaynak ListesiğŸŠ\n",
        "\n",
        "\n",
        "| No | Kurs AdÄ±                                           | Ãœniversite/ EÄŸitmen(ler)                        | Ders WebSayfasÄ±                                               | Ders VideolarÄ±                                               | YÄ±l            |\n",
        "| ---- | ----------------------------------------------------- | ----------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | --------------- |\n",
        "| 1.   | **Neural Networks for Machine Learning** | Geoffrey Hinton, University of Toronto          | [Lecture-Slides](http://www.cs.toronto.edu/~hinton/coursera_slides.html) <br/> [CSC321-tijmen](https://www.cs.toronto.edu/~tijmen/csc321/) | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9) <br/> [UofT-mirror](https://www.cs.toronto.edu/~hinton/coursera_lectures.html) | 2012 <br/> 2014 |\n",
        "| 2.   | **Neural Networks Demystified**| Stephen Welch, Welch Labs                       | [Suppl. Code](https://github.com/stephencwelch/Neural-Networks-Demystified) | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU) | 2014            |\n",
        "| 3.   | **Deep Learning at Oxford** | Nando de Freitas, Oxford University             | [Oxford-ML](http://www.cs.ox.ac.uk/teaching/courses/2014-2015/ml/) | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu) | 2015            |\n",
        "| 4.   | **CS231n: CNNs for Visual Recognition** | Andrej Karpathy, Stanford University            | [CS231n](http://cs231n.stanford.edu/2015/)                   | `None`                                                       | 2015            |\n",
        "| 5.   | **CS231n: CNNs for Visual Recognition**| Andrej Karpathy, Stanford University            | [CS231n](http://cs231n.stanford.edu/2016/)                   | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC) | 2016            |\n",
        "| 6.   | **CS231n: CNNs for Visual Recognition**| Justin Johnson, Stanford University             | [CS231n](http://cs231n.stanford.edu/2017/)                   | [YouTube-Lectures](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv) | 2017            |\n",
        "| 7.   | **CS224d: Deep Learning for NLP** | Richard Socher, Stanford University             | [CS224d](http://cs224d.stanford.edu)                         | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLmImxx8Char8dxWB9LRqdpCTmewaml96q) | 2015            |\n",
        "| 8.   | **CS224d: Deep Learning for NLP** | Richard Socher, Stanford University             | [CS224d](http://cs224d.stanford.edu)                         | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLlJy-eBtNFt4CSVWYqscHDdP58M3zFHIG) | 2016            |\n",
        "| 9.   | **CS224n: NLP with Deep Learning** | Richard Socher, Stanford University             | [CS224n](http://web.stanford.edu/class/cs224n/)              | [YouTube-Lectures](https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6) | 2017            |\n",
        "| 10.  | **Neural Networks** | Hugo Larochelle, UniversitÃ© de Sherbrooke       | [Neural-Networks](http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html) | [YouTube-Lectures](https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH) | 2016            |\n",
        "| 11.  | **Deep Learning**  | Andrew Ng, Stanford University                  | [CS230](http://cs230.stanford.edu/)                          | `None`                                                       | 2018            |\n",
        "|      |                                                       |                                                 |                                                              |                                                              |                 |\n",
        "| 12.  | **Bay Area Deep Learning** | Many legends, Stanford                          | `None`                                                       | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLrAXtmErZgOfMuxkACrYnD2fTgbzk2THW) | 2016            |\n",
        "| 13.  | **UvA Deep Learning**| Efstratios Gavves, University of Amsterdam(UvA) | [UvA-DLC](https://uvadlc.github.io/)                         | [Lecture-Videos](https://uvadlc.github.io/#lectures)         | 2018            |\n",
        "| 14.  | **Advanced Deep Learning and Reinforcement Learning** | Many legends, DeepMind                          | `None`                                                       | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLqYmG7hTraZDNJre23vqCGIVpfZ_K2RZs) | 2018            |\n",
        "| 15.  | **Deep Learning**  | Francois Fleuret, EPFL                          | [EE-59](https://fleuret.org/ee559/)                          | `None`                                                       | 2019            |\n",
        "| 16.  | **Deep Learning**  | Francois Fleuret, EPFL                          | [EE-59](https://fleuret.org/ee559-2018/dlc)                  | [Video-Lectures](https://fleuret.org/ee559-2018/dlc/#materials) | 2018            |\n",
        "| 17.  | **Deep Learning for Perception**| Dhruv Batra, Virginia Tech                      | [ECE-6504](https://computing.ece.vt.edu/~f15ece6504/)        | [YouTube-Lectures](https://www.youtube.com/playlist?list=PL-fZD610i7yAsfH2eLBiRDa90kL2ML0f7) | 2015            |\n",
        "| 18.  | **Introduction to Deep Learning**  | Alexander Amini, Harini Suresh and others, MIT  | [6.S191](http://introtodeeplearning.com/)                    | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI) <br/> [2017-version](https://www.youtube.com/playlist?list=PLkkuNyzb8LmxFutYuPA7B4oiMn6cjD6Rs) | 2017- 2019      |\n",
        "| 19.  | **Deep Learning for Self-Driving Cars**  | Lex Fridman, MIT                                | [6.S094](https://selfdrivingcars.mit.edu/)                   | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf) | 2017-2018       |\n",
        "| 20.  | **MIT Deep Learning** | Many Researchers,  Lex Fridman, MIT             | [6.S094, 6.S091, 6.S093](https://deeplearning.mit.edu/)      | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf) | 2019            |\n",
        "| 21.  | **Introduction to Deep Learning**| Bhiksha Raj and many others, CMU                | [11-485/785](http://deeplearning.cs.cmu.edu/)                | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLp-0K3kfddPwJBJ4Q8We-0yNQEG0fZrSa) | S2018           |\n",
        "| 22.  | **Introduction to Deep Learning**  | Bhiksha Raj and others, CMU                     | [11-485/785](http://deeplearning.cs.cmu.edu/)                | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLp-0K3kfddPyH44FP0dl0CbYprvTcfgOI)   [Recitation-Inclusive](https://www.youtube.com/playlist?list=PLLR0_ZOlbfD6KDBq93G8-guHI-J1ICeFm) | F2018           |\n",
        "| 23.  | **Deep Learning Specialization**  | Andrew Ng, Stanford                             | [DL.AI](https://www.deeplearning.ai/deep-learning-specialization/) | [YouTube-Lectures](https://www.youtube.com/channel/UCcIXc5mJsHVYTZR1maL5l9w/playlists) | 2017-2018       |\n",
        "| 24.  | **Deep Learning**  | Ali Ghodsi, University of Waterloo              | [STAT-946](https://uwaterloo.ca/data-analytics/deep-learning) | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLehuLRPyt1Hyi78UOkMPWCGRxGcA9NVOE) | F2015           |\n",
        "| 25.  | **Deep Learning**  | Ali Ghodsi, University of Waterloo              | [STAT-946](https://uwaterloo.ca/data-analytics/teaching/deep-learning-2017) | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLehuLRPyt1HxTolYUWeyyIoxDabDmaOSB) | F2017           |\n",
        "| 26.  | **Deep Learning** | Mitesh Khapra, IIT-Madras                       | [CS7015](https://www.cse.iitm.ac.in/~miteshk/CS7015.html)    | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLyqSpQzTE6M9gCgajvQbc68Hk_JKGBAYT) | 2018            |\n",
        "| 27.  | **Deep Learning for AI** | UPC Barcelona                                   | [DLAI-2017](https://telecombcn-dl.github.io/2017-dlai/) <br/> [DLAI-2018](https://telecombcn-dl.github.io/2018-dlai/) | [YouTube-Lectures](https://www.youtube.com/playlist?list=PL-5eMc3HQTBagIUjKefjcTbnXC0wXC_vd) | 2017-2018       |\n",
        "|      |                                                       |                                                 |                                                              |                                                              |                 |\n",
        "| 28.  | **Deep Learning Book** companion videos| Ian Goodfellow and others                       | [DL-book slides](https://www.deeplearningbook.org/lecture_slides.html) | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLsXu9MHQGs8df5A4PzQGw-kfviylC-R9b) | 2017            |\n",
        "| 29.  | **Neural Networks**   | Grant Sanderson                                 | `None`                                                       | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) | 2017-2018       |\n",
        "| 30.  | **Deep Learning**  | Alex Bronstein and Avi Mendelson, Technion      | [CS236605](https://vistalab-technion.github.io/cs236605/info/) | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLM0a6Z788YAZuqg2Ip-_dPLzEd33lZvP2) | 2018            |\n",
        "| 31.  | **Yapay Zekada Ã‡arpÄ±cÄ± Yenilik: KapsÃ¼l AÄŸlarÄ±**  | M. AyyÃ¼ce KÄ±zrak - YAZGÄ°G-2018/ Ankara     | [KapsÃ¼l AÄŸlarÄ±](https://colab.research.google.com/github/ayyucekizrak/Udemy_DerinOgrenmeyeGiris/blob/master/KapsulAglari/KapsulAglari_MNIST.ipynb) | [YouTube-Lectures](https://www.youtube.com/watch?v=vkheAJgKJa4) | 2018            |\n",
        "| 32.  | **Google Colab ile Ãœcretsiz GPU KullanÄ±mÄ±**     | M. AyyÃ¼ce KÄ±zrak - Udemy - Deep Learning A-Zâ„¢| Python ile Derin Ã–ÄŸrenme   | [Google Colab ile Ãœcretsiz GPU KullanÄ±mÄ±](https://colab.research.google.com/github/ayyucekizrak/Udemy_DerinOgrenmeyeGiris/blob/master/Python%20Numpy%20Giris/ilkadim.ipynb) | [YouTube-Lectures](https://www.youtube.com/watch?v=bT-T1i_Rpy8&t=3s) | 2018            |\n",
        "| 33.  | **Google Colab Free GPU Tutorial**  | Fuat BeÅŸer - Deep Learning Turkey - Medium Blog   | [Google Colab Free GPU Tutorial](https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d) | [YouTube-Lectures](https://www.youtube.com/watch?v=bT-T1i_Rpy8&t=3s) | 2018            |\n",
        "| 34.  | **Derin Ã–ÄŸrenme ile Artistik Stil Transferi**   | M. AyÃ¼ce KÄ±zrak - Udemy - Deep Learning A-Zâ„¢| Python ile Derin Ã–ÄŸrenme  | [Derin Ã–ÄŸrenme ile Artistik Stil Transferi](https://medium.com/deep-learning-turkiye/derin-%C3%B6%C4%9Frenme-ile-artistik-stil-transferi-29256789c7e8) | [YouTube-Lectures](https://www.youtube.com/watch?v=fM28e7o6CJc) | 2018            |\n",
        "| 35.  | **Derin Ã–ÄŸrenme Ä°Ã§in Aktivasyon FonksiyonlarÄ±nÄ±n KarÅŸÄ±laÅŸtÄ±rÄ±lmasÄ±**                            | M. AyÃ¼ce KÄ±zrak - Deep Learning TÃ¼rkiye - Medium Blog  | [Derin Ã–ÄŸrenme Ä°Ã§in Aktivasyon FonksiyonlarÄ±nÄ±n KarÅŸÄ±laÅŸtÄ±rÄ±lmasÄ±](https://medium.com/deep-learning-turkiye/derin-%C3%B6%C4%9Frenme-i%C3%A7in-aktivasyon-fonksiyonlar%C4%B1n%C4%B1n-kar%C5%9F%C4%B1la%C5%9Ft%C4%B1r%C4%B1lmas%C4%B1-cee17fd1d9cd) | [YouTube-Lectures](https://www.youtube.com/watch?v=ZMkLC-ebIqE&t=407s) | 2019            |\n",
        "|   |                             |   |  |  |          |\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}